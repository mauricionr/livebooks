# Traditional machine learning

```elixir
Mix.install([
  {:scholar, "~> 0.2.1"},
  {:nx, "~> 0.6.2"},
  {:exla, "~> 0.6.1"},
  {:vega_lite, "~> 0.1.8"},
  {:kino_vega_lite, "~> 0.1.10"},
  {:scidata, "~> 0.1.11"}
])
```

## Section

```elixir
Nx.default_backend(EXLA.Backend)
```

```elixir
Nx.Defn.default_options(compiler: EXLA)
```

```elixir
m = :rand.uniform() * 10
```

```elixir
b = :random.uniform() * 10
```

```elixir
key = Nx.Random.key(42)
```

```elixir
size = 100
```

```elixir
{x, new_key} = Nx.Random.normal(key, 0.0, 1.0, shape: {size, 1})
```

```elixir
{noise_x, new_key} = Nx.Random.normal(new_key, 0.0, 1.0, shape: {size, 1})
```

```elixir
y = m |> Nx.multiply(Nx.add(x, noise_x)) |> Nx.add(b)
```

```elixir
alias VegaLite, as: VL
```

```elixir
VL.new(title: "Scatterplot", width: 720, height: 480)
|> VL.data_from_values(%{
  x: Nx.to_flat_list(x),
  y: Nx.to_flat_list(y)
})
|> VL.mark(:point)
|> VL.encode_field(:x, "x", type: :quantitative)
|> VL.encode_field(:y, "y", type: :quantitative)
```

```elixir
model = Scholar.Linear.LinearRegression.fit(x, y)
```

```elixir
Scholar.Linear.LinearRegression.predict(model, Nx.iota({3, 1}))
```

```elixir
pred_xs = Nx.linspace(-3.0, 3.0, n: 100) |> Nx.new_axis(-1)
```

```elixir
pred_ys = Scholar.Linear.LinearRegression.predict(model, pred_xs)
```

```elixir
VL.new(title: "Scatterplot Distribution and fit curve", width: 720, height: 480)
|> VL.data_from_values(%{
  x: Nx.to_flat_list(x),
  y: Nx.to_flat_list(y),
  pred_x: Nx.to_flat_list(pred_xs),
  pred_y: Nx.to_flat_list(pred_ys)
})
|> VL.layers([
  VL.new()
  |> VL.mark(:point)
  |> VL.encode_field(:x, "x", type: :quantitative)
  |> VL.encode_field(:y, "y", type: :quantitative),
  VL.new()
  |> VL.mark(:line)
  |> VL.encode_field(:x, "pred_x", type: :quantitative)
  |> VL.encode_field(:y, "pred_y", type: :quantitative)
])
```

## Logistic regression with scholar

```elixir
{inputs, targets} = Scidata.Wine.download()
```

```elixir
{train, test} =
  inputs
  |> Enum.zip(targets)
  |> Enum.shuffle()
  |> Enum.split(floor(length(inputs) * 0.8))
```

```elixir
{train_inputs, train_targets} = Enum.unzip(train)
```

```elixir
train_inputs = Nx.tensor(train_inputs)
```

```elixir
train_targets = Nx.tensor(train_targets)
```

```elixir
{test_inputs, test_targets} = Enum.unzip(test)
```

```elixir
test_inputs = Nx.tensor(test_inputs)
```

```elixir
test_targets = Nx.tensor(test_targets)
```

```elixir
train_inputs = Scholar.Preprocessing.min_max_scale(train_inputs)
```

```elixir
test_inputs = Scholar.Preprocessing.min_max_scale(test_inputs)
```

```elixir
model = Scholar.Linear.LogisticRegression.fit(train_inputs, train_targets, num_classes: 3)
```

```elixir
test_preds = Scholar.Linear.LogisticRegression.predict(model, test_inputs)
```

```elixir
Scholar.Metrics.Classification.accuracy(test_targets, test_preds)
```

```elixir
Scholar.Metrics.Classification.confusion_matrix(test_targets, test_preds, num_classes: 3)
```

```elixir
VL.new(title: "Confusion matrix", width: 720, height: 480)
|> VL.data_from_values(%{
  predicted: Nx.to_flat_list(test_preds),
  actual: Nx.to_flat_list(test_targets)
})
|> VL.mark(:rect)
|> VL.encode_field(:x, "predicted")
|> VL.encode_field(:y, "actual")
|> VL.encode(:color, aggregate: :count)
```

```elixir

```
